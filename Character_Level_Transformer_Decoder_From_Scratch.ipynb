{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This project walks through Andrej Karpathy's work at: https://www.youtube.com/watch?v=kCc8FmEb1nY to build a small transformer trained with the TinyShakespeare dataset \n",
        "\n",
        "We will use TinyShakespeare... a dataset containing all of Shakespeare's works. \n",
        "\n",
        "This can be found: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "The goal will be to train a Transformer-based architecture that creates an \"infinite\" Shakespeare generator \n",
        "\n",
        "\n",
        "Plan: \n",
        "- define a transformer architecture\n",
        "- train on the TinyShakespeare dataset \n",
        "- Generate infinite shakespeare \n",
        "\n"
      ],
      "metadata": {
        "id": "Bt54PLNXcQfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytw1ro7TcjZY",
        "outputId": "c7f8fb4d-8b01-45ea-be32-dfbbac651644"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 04:18:11--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-01-22 04:18:12 (114 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding = 'utf-8') as f: \n",
        "  text = f.read()\n"
      ],
      "metadata": {
        "id": "946sW1uOd_lP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('length of dataset in characters: ', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UrThSH2ePKp",
        "outputId": "535e8580-23d1-4f05-ff04-26b61b146164"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the length of the dataset is a little over 1M characters"
      ],
      "metadata": {
        "id": "huZAfo6BeXHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the first 100 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vp-X6VgeV0t",
        "outputId": "f2da2f65-87f9-4b9d-f762-c77dd7e73750"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#determine all the unique characters that are present in the text\n",
        "\n",
        "unique_chars = sorted(list(set(text)))\n",
        "vocab_size = len(unique_chars) #vocab size defines the possible elements of our sequences\n",
        "\n",
        "print(''.join(unique_chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcvzByDmegLE",
        "outputId": "6e4748f8-2139-483e-ca10-4e20619cc537"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see there are 65 total characters that the model can see or emit \n",
        " "
      ],
      "metadata": {
        "id": "KXwAPfnqfAo8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjZ0P1bvet1u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizing the input text\n",
        "\n",
        "Convert the raw text (as string) to a sequence of integers, according to some vocabulary \n",
        "\n",
        "Since we are building a character level language model, we will transfer individual characters to integers: eg. \"a\" maps to \"5\"; \"b\" maps to \"6\", etc. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "457P2ckUfHNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate over all characters and create a map from the character to the integer, and vice versa \n",
        "string_to_ints = {ch: i for i, ch in enumerate(unique_chars)}\n",
        "ints_to_strings = {i:ch for i, ch in enumerate(unique_chars)}\n",
        "\n",
        "#encoding: taking a string and outputting a list of ints. \n",
        "encode = lambda s: [string_to_ints[c] for c in s]\n",
        "#decoding: the opposite, take a list of integers and output a string  \n",
        "decode = lambda l: ''.join(ints_to_strings[i] for i in l)\n",
        "\n",
        "#test out on an example\n",
        "print(encode('hello, how are you?'))\n",
        "print(decode(encode('hello, how are you?')))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q6vML9efoZj",
        "outputId": "298038d9-f898-4e12-84ee-111d17dedd8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 6, 1, 46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59, 12]\n",
            "hello, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have encoded a string, and decoded it back... \n",
        "\n",
        "There are many other encoders/decoders we can use. Eg. SentencePiece, which encodes at the sub-word level (between characters and words). \n",
        "\n",
        "GPT uses byte-word \n",
        "\n",
        "we can trade off between sequence length and vocabulary size: eg. large vocabulary size with small sequence length..\n",
        "\n",
        "We will use simple functions, so we will get long sequences and small vocabulary size"
      ],
      "metadata": {
        "id": "4bba37Xkh3eI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can tokenize the entire Shakespeare training set\n",
        "\n",
        "we will use the Pytorch tensor "
      ],
      "metadata": {
        "id": "nEfN4h2XiW41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encode the text and wrap it in a Pytorch tensor\n",
        "\n",
        "import torch \n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI1A5HwZhb8h",
        "outputId": "1354b25e-9824-488d-a1d4-9b0fe0b97365"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sequence of the first 1000 characters encoded as integers, in the form of a Pytorch Tensor\n",
        "\n",
        "The entire text is represented as a sequence of integers\n",
        "\n",
        "Now, we want to do a train/test split at 90%/10%, respectively"
      ],
      "metadata": {
        "id": "C9CILc2ci2q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(.9* len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "4b8fMlmqir4N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't feed all the data in to the Transformer at once... we need to feed in small chunks (of a maximum length: Block_size / context_length ) at random "
      ],
      "metadata": {
        "id": "LZYV4bGajbGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 \n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVKa3SHvjWfU",
        "outputId": "7916f0b9-20b3-4b3b-df84-a2847e1b22b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the first 9 characters in the training set \n",
        "\n",
        "In these 9 characters, there are 8 individual training examples: \n",
        "\n",
        "eg. in the context of 18, 47 comes next. In the context of 18 and 47, 56 comes next, etc. "
      ],
      "metadata": {
        "id": "W-nViYMBjqfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x are inputs to transformer ... the first block size characters\n",
        "x = train_data[:block_size]\n",
        "#y are the targets for each position in the input... they will be next block size, (offset by 1 \n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1] \n",
        "  target = y[t]\n",
        "  print(f'when input is {context} the target is: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxQ0m1HUjoqo",
        "outputId": "4ce5a4e7-f352-4d85-d976-c397fced30a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is: 47\n",
            "when input is tensor([18, 47]) the target is: 56\n",
            "when input is tensor([18, 47, 56]) the target is: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This spells out what we said above: \n",
        "\n",
        "there are 8 contexts: 18; 18, 47; 18, 47, 56, etc. \n",
        "\n",
        "and there are 8 targets (eg. tokens that come next, and that we are aiming to predict): 47, 56, 57, respectively "
      ],
      "metadata": {
        "id": "ZDTF4G9lk6TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to think about batch_size, to process multiple chunks in parallel on the GPU "
      ],
      "metadata": {
        "id": "UeSN9GiNmQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8 \n",
        "\n",
        "#generate a small batch of data of inputs x and targets y \n",
        "#we will be stacking 4 rows of width 8 into a single 4x8 tensor\n",
        "\n",
        "def get_batch(split): \n",
        "  #set the data that we are grabbing the batches from to be train_data or test_data\n",
        "  data = train_data if split == 'train' else val_data \n",
        "  #set batch_size number of indexes for where to grab the chunks from in the data array\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  #grab batches of data for inputs, by concatenation  \n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  #grab batches of data for targets, which will be offset by 1 compared to x \n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs: ')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets: ')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "#some code to help understand the context and targets a bit more \n",
        "\n",
        "for b in range(batch_size): #batch dimension \n",
        "  for t in range(block_size): \n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f'when input is {context.tolist()} the target: {target}')\n",
        " \n",
        "\n",
        " #we have 32 independent examples packed into a single batch "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYFQq2Djk4pH",
        "outputId": "16f604cc-24ca-4f23-af28-21c58c7d2106"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "-----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print our input to the transformer \n",
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eFdFPRcrrFb",
        "outputId": "98480ba5-7417-4c0e-b610-828f54588eaf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJUJMp2hrveQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start with simple baseline: Bigram language model using a Pytorch module \n",
        "\n"
      ],
      "metadata": {
        "id": "6AQqkBb3rw-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): \n",
        "  def __init__(self): \n",
        "    super().__init__()\n",
        "    #each token \n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #Embedding class creates a tensor of vocab_size x vocab_size \n",
        "\n",
        "  def forward(self, idx, targets = None): \n",
        "    #idx and targets are both (Batch_size, Time) tensors of integers\n",
        "    #Logits are of size (B,T,C); with C = vocab_size ... these are the predictions for each one of the 4x8 positions \n",
        "    #in other words, for each batch, for each position context, there is a list of predictions at that position \n",
        "    logits = self.token_embedding_table(idx) \n",
        "\n",
        "    #targets is optional, so if there is a targets inputted: \n",
        "    if targets is None: \n",
        "      loss = None\n",
        "\n",
        "    else: \n",
        "      #build a loss\n",
        "      #pytorch wants (B,C,T) rather than (B,T,C), so we need to reshape logits \n",
        "      B,T,C = logits.shape\n",
        "      #reshape logits to a shape that pytorch expects\n",
        "      logits = logits.view(B*T, C) #stretch out the 3D tensor into a 2D tensor, preserving the channels as the 2nd dimension \n",
        "      #reshape targets: they are currently (B,T), we will stretch to make 1D)  \n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  #continues the generation in the time dimension, for each batch dimension \n",
        "  def generate(self, idx, max_new_tokens): #max new tokens is a parameter determining the number of tokens we want to generate \n",
        "    #idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens): \n",
        "      logits, loss = self(idx) # shape(B,T,C) this will perform the forward function \n",
        "      #focus only on the last time step, the prediction for the next token \n",
        "      logits = logits[:, -1, :] #this becomes (B,C) \n",
        "      #apply softmax over the C dimension to get probabilities \n",
        "      probs = F.softmax(logits, dim = -1) #(B,C) \n",
        "      #sample 1 item from this distribution \n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1) \n",
        "      #append sampled index to the running sequence \n",
        "      idx = torch.cat((idx, idx_next), dim = 1) #(B,T+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "#generate a (random, because untrained) length 100 sequence from the model, by inputting a single 'space' character\n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0-jz_Lsr1W8",
        "outputId": "89c10913-0741-4f4a-a380-d935eb1dc79c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W4Zyk8Qn06X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model so far is a bit outrageous, because we are feeding long (block_size) length contexts into the generate function, but the function is only making predictions using the token immediately preceding the token to predict on.... We are doing this so we can re-use the generate function later on.  "
      ],
      "metadata": {
        "id": "wWddrEPpuO-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model\n",
        "\n"
      ],
      "metadata": {
        "id": "Lqz6LKdb0yFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a pytorch optimizer \n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n"
      ],
      "metadata": {
        "id": "gQ4ty7_SuN5E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): \n",
        "  #get a batch of data \n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss \n",
        "  logits, loss = m(xb, yb) #pass the index and targets thorugh our bigram model \n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPiBJmbg1UHN",
        "outputId": "014a19b2-10e9-4fef-e787-7f686f28e787"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.382369041442871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's generate some predictions and decode \n",
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 300)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzuP7ivF1y7K",
        "outputId": "9545c9ae-0087-4cc4-9c45-670ca4c37ad5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
            "RIfans picspeserer hee tha,\n",
            "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
            "ELIS me hurf lal y, ma dus pe athouo\n",
            "BEY:! Indy; by s afreanoo adicererupa anse tecor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still jibberish, but starting to look almost like English... definitely not shakespeare \n",
        "\n",
        "We are only lookig at very last character... now we want to use more context, more history... \n",
        "\n",
        "so we will build a Transformer \n"
      ],
      "metadata": {
        "id": "lrnOCtwj2QOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But first, let's build a Python script out of everything we have written so far...\n",
        "\n",
        "Almost everything is the same, except we have added: \n",
        "- GPU-optimization \n",
        "- A more accurate loss estimation that estimates the loss over a whole batch \n",
        "\n"
      ],
      "metadata": {
        "id": "RAHKd8MO356_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'bigram.py'\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #32 dimensional embeddings \n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #each position will get it's own embedding vector \n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) #create a linear layer \n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T, device = device)) # (T, C) \n",
        "        x = tok_embd + pos_embd #now x will include both the token identities and their positions\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAxVjOgd2H7d",
        "outputId": "c03eaefe-774d-407c-a2b4-b3b0f6b10b9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing bigram.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OADudte04wXv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Self Attention Toy Example"
      ],
      "metadata": {
        "id": "iRfOO4uP40oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0LfWn8_sFpI",
        "outputId": "56ec197d-eb87-4590-91fd-c41685618d02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#VERSION 1 \n",
        "\n",
        "#We will implement a simple way of including the context of previous tokens when making a prediction - by simply averaging over all prior tokens to a given token\n",
        "# we want x{b,t] = mean_{i<= t} x[b,i] \n",
        "xbow = torch.zeros((B,T,C)) #xbow refers to a bag of words - ie. average\n",
        "for b in range(B): \n",
        "  for t in range(T): \n",
        "    xprev = x[b, :t+1] #(t,C) #this gives you tokens previous to a given token\n",
        "    xbow[b,t] = torch.mean(xprev, 0) #average over the 0th dimension (time) \n"
      ],
      "metadata": {
        "id": "N5UfHmCfsFaR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to elucidate this, let's index into the first batch, and print out x and xbow\n",
        "print(x[0]) \n",
        "\n",
        "print(xbow[0])"
      ],
      "metadata": {
        "id": "nqev86eKtikO",
        "outputId": "1e760cbe-1095-4d7c-8748-0203ed177cf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2794, -0.7354],\n",
            "        [-0.2471,  0.9398],\n",
            "        [ 2.0026, -1.3095],\n",
            "        [ 0.3397,  0.2184],\n",
            "        [-1.9841,  0.3335],\n",
            "        [ 0.4053,  1.8873],\n",
            "        [ 1.2212, -1.2814],\n",
            "        [ 0.7391,  0.6104]])\n",
            "tensor([[ 1.2794, -0.7354],\n",
            "        [ 0.5162,  0.1022],\n",
            "        [ 1.0117, -0.3683],\n",
            "        [ 0.8437, -0.2217],\n",
            "        [ 0.2781, -0.1106],\n",
            "        [ 0.2993,  0.2224],\n",
            "        [ 0.4310,  0.0075],\n",
            "        [ 0.4695,  0.0829]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the first element remains unchanged, since it is simply an average of the first element itself. \n",
        "The second row of xbow, however, is an average of the first and second row of x, etc. \n",
        "The last row is the average over all the rows. \n",
        "\n",
        "We can be more efficient with Matrix multiplication \n"
      ],
      "metadata": {
        "id": "8_KuX71jt3N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VERSION 2 \n",
        "\n",
        "wei = torch.tril(torch.ones(T,T)) #doing weighted sums with matrix multiplication \n",
        "wei = wei / torch.sum(wei, 1, keepdim = True)\n",
        "xbow2 = wei @ x # (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
        "wei "
      ],
      "metadata": {
        "id": "121CLvqcw1cZ",
        "outputId": "ae755321-38e6-4846-927e-60a1a4f8c08b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#showing that xbow and xbow2 are equivalent\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "id": "5m4oj7UwzQGP",
        "outputId": "03d230ae-501a-413d-ce93-1ebe0ff98d02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tW_Js5ZoxIMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VERSION 3 \n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T)) #acts as affinities between the tokens. Will eventually be data dependent - different tokens will start to find other tokens more or less interesting \n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) #wherever the values of tril = 0, fill wei with -inf ... like saying the future cannot communicate with the past  \n",
        "wei = F.softmax(wei, dim = 1) #applying a softmax will essentially normalize each row, so that we get a matrix that is equivalent to our wei matrix above ... \n",
        "print(wei)\n",
        "xbow3 = wei @ x \n",
        "\n",
        "#show that xbow3 is arbitrarily close to xbow \n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "id": "z_sVT07Ntx9H",
        "outputId": "b6c523fc-f1d0-49d7-8054-729794dd1a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 4: self-attention \n",
        "\n",
        "torch.manual_seed(1337) \n",
        "B,T,C = 4, 8, 32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "#let's see a single head perform self attention \n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False) \n",
        "\n",
        "#'self' attention because the keys, queries, and values are all coming from the same source - x ... 'cross attention' would potentially use keys, and values from another source\n",
        "k = key(x) #(B,T,16) #this represents a token's \"key\"\n",
        "q = query(x) #(B,T,16) #this represents a token's \"query\"\n",
        "#this is performing a dot product between each token's queries and all the other tokens' keys, a measure of how much affinity the differnet tokens should have for each other... in other words \n",
        "wei = q @ k.transpose(-2, -1) # (B,T,C) @ (B,16,T) --> (B,T,T)  \n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#Since we are recreating a 'decoder' block, this makes tokens only use as context previous tokens, not future tokens . we would not use this in an encoder block \n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  \n",
        "wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "v = value(x) \n",
        "out = wei @ v\n",
        "\n",
        "out.shape\n"
      ],
      "metadata": {
        "id": "1YgNsCHTvtYp",
        "outputId": "ab0edb03-19b0-4c17-988c-9e7601215025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We will finish off by making a script out of everything we have so far. We will add several novel components, including: \n",
        "- Multihead attention: multiple self-attention blocks running in parallel \n",
        "- Add layer normalization: this normalizes across the embedding dimensino for each token, forcing activations to be mean 0 and 1 STD \n",
        "- Add dropout layers: randomly dropout some subset of node activitiy on each pass during training, effectively creating sub-network clusters\n",
        "- Add multiple transformer decoder blocks: stack multiple of the transformer decoder blocks that we have previously encoded.  \n",
        "  - Within these blocks, add residual connections: skip connections "
      ],
      "metadata": {
        "id": "lnwX7Sgb-emP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK... let's create a script out of everything we have so far: \n",
        "\n"
      ],
      "metadata": {
        "id": "RYMOzxWD-PEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile final_shakespeare_transformer.py \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd) #add a layer norm \n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) #residual connections\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "id": "XZuCAC1l-OzD",
        "outputId": "6041e086-da17-492e-874b-430802933f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing self_attention.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4xhsA29CvwPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtglNKF0-nBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}